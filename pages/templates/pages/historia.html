{% extends 'pages/base.html' %}
{% block content %}

    <div class="his_wstep">
      <h2>Wstęp</h2>
      <p>Historia sztucznej inteligencji ( AI ) rozpoczęła się w starożytności , z mitami, opowieściami i pogłoskami o sztucznej
      istocie obdarzonej inteligencją lub świadomością przez mistrzów; jak pisze Pamela McCorduck  "AI zaczęło się od
      "starożytnej chęci wykuwania bogów".

Nasiona współczesnej sztucznej inteligencji zasadzili klasyczni filozofowie, którzy próbowali opisać proces ludzkiego myślenia
jako mechaniczną manipulację symbolami. Ta praca zakończyła się wynalezieniem programowalnego komputera cyfrowego w latach 40.
XX wieku, maszyną opartą na abstrakcyjnej esencji matematycznego rozumowania. To urządzenie i pomysły stojące za nim zainspirowały
garstkę naukowców do poważnego przedyskutowania możliwości zbudowania elektronicznego mózgu.

Dziedzina badań nad sztuczną inteligencją została założona podczas warsztatów odbywających się w kampusie Dartmouth College w
lecie 1956 roku. Ci, którzy brali udział, stali się liderami badań nad sztuczną inteligencją od dziesięcioleci. Wielu z nich
przewidziało, że maszyna tak inteligentna jak człowiek będzie istniała w nie więcej niż jednym pokoleniu i otrzymają miliony
dolarów na realizację tej wizji.

Ostatecznie stało się oczywiste, że poważnie nie docenili trudności projektu ze względu na ograniczenia sprzętowe komputera.
W 1973 r., w odpowiedzi na krytykę Jamesa Lighthilla i ciągłe naciski ze strony kongresu, rządy USA i Wielkiej Brytanii przestały
finansować badania nad sztuczną inteligencją, a trudne lata, które nastąpiły później, były nazywane " zimową sztuczną
inteligencją ". Siedem lat później, wizjonerska inicjatywa rządu japońskiego zainspirowała rządy i przemysł do zapewnienia AI
miliardów dolarów, ale pod koniec lat 80. inwestorzy zaczęli odczuwać rozczarowanie brakiem potrzebnej mocy komputerowej
(sprzęt komputerowy) i ponownie wycofywali fundusze.

Inwestycje i zainteresowanie sztuczną inteligencją wzrosły w pierwszych dziesięcioleciach XXI wieku, kiedy uczenie maszynowe
zostało z powodzeniem zastosowane w wielu problemach w środowisku akademickim i przemysłowym z powodu obecności potężnego sprzętu
komputerowego. Podobnie jak w poprzednich " AI summers ", niektórzy obserwatorzy (tacy jak Ray Kurzweil [2] ) przewidzieli
nadejście sztucznej ogólnej inteligencji : maszynę o zdolnościach intelektualnych przekraczających możliwości człowieka.</p>
    </div>
    <div class="">
      <h2>Początki</h2>
      Marzenia o sztucznej inteligencji został po raz pierwszy pojawiły się u filozoów indyjskich, takich jak te Charvace ,
      w tradycjach filozoficznych sięgających 1500 r. p.n.e. i zachowanych dokumentów około 600 rpne. McCorduck (2004) pisze, że
      "sztuczna inteligencja w takiej czy innej formie jest ideą, która przenika intelektualną historię, marzenie w nagłej
      potrzebie zrealizowania", wyrażoną w mitach, legendach, opowieściach, spekulacjach i automatach do gier w ludzkości.
    </div>
    <div class="">
      <h2>Sztuczna inteligencja w mitach, spekulacjach i księgach</h2>
      Mechaniczni ludzie i sztuczne istoty pojawiają się w greckich mitach , takich jak złote roboty Hefajstosa i Galatea
      Pygmaliona.
      W średniowieczu krążyły pogłoski o tajnych mistycznych lub alchemicznych sposobach umieszczania umysłu w materii, takichjak
      Takab Dżabir Ibn Hayyana , Homunkulus Paracelsusa i golem Rabiego Judah Loew. Do XIX wieku pomysły dotyczące sztucznych
      mężczyzn i maszyn myślących zostały opracowane w fikcji, podobnie jak w przypadku Frankensteina autorstwa Mary Shelley i spekulacji,
      takich jak u Samuela Butlera w "Darwin wśród maszyn". SI nadal jest ważnym elementem fantastyki naukowej w dzisiejszym
      świecie.
    </div>
    <div class="">
      <h2>Automatony</h2>
      Realistyczne humanoidalne automaty zostały zbudowane przez rzemieślników z każdej cywilizacji, w tym Yan Shi, Hero z Aleksandrii
      Al-Dżazariego, Pierre Jaquet-Droz i Wolfganga von Kempelena. Najstarsze znane automaty to święte posągi starożytnego Egiptu
       i Grecji . Wierni wierzyli, że rzemieślnik nasycił te postacie bardzo prawdziwymi umysłami, zdolnymi do mądrości i emocji
       - Hermes Trismegistus napisał, że "odkrywając prawdziwą naturę bogów, człowiek był w stanie ją odtworzyć."
    </div>
    <div class="">
      <h2>Formalne rozumowanie</h2>
      Sztuczna inteligencja opiera się na założeniu, że proces ludzkiej myśli można zmechanizować. Studiowanie rozumowania
       mechanicznego lub formalnego ma długą historię. Filozofowie chińscy , indyjscy i greccy opracowali uporządkowane metody
       formalnego odliczenia w pierwszym tysiącleciu pne. Ich idee rozwijały się przez wieki przez filozofów takich jak
       Arystoteles (który przedstawił formalną analizę sylogizmu ), Euklidesa (którego Elementy stanowiły wzór formalnego
       rozumowania), al-Khwārizmī (który opracował algebrę i nadał jej nazwę " algorytmowi ") ) i europejscy scholastycy,
       tacy jak William Ockham i Duns Scotus.

Filozof majorkański Ramon Llull (1232-1315) opracował kilka logicznych maszyn poświęconych wytwarzaniu wiedzy metodami
logicznymi; Llull opisał swoje maszyny jako mechaniczne jednostki, które mogłyby łączyć podstawowe i niezaprzeczalne
prawdy za pomocą prostych logicznych operacji, wytwarzanych przez maszynę za pomocą mechanicznych znaczeń, w taki sposób,
aby wytworzyć całą możliwą wiedzę. Praca Llulla miała wielki wpływ na Gottfrieda Leibniza , który przebudował swoje
pomysły.
Gottfried Leibniz , który spekulował, że ludzki rozum może zostać zredukowany do mechanicznych obliczeń

W XVII wieku Leibniz , Thomas Hobbes i René Descartes badali możliwość, że wszystkie racjonalne myśli można uczynić tak
systemowymi jak algebra lub geometria. Hobbes napisał w Lewiatannie : "rozum jest niczym innym, jak rachubą". Leibniz
przewidywał uniwersalny język rozumowania (jego characteristica universalis ), który ograniczyłby argumentację do kalkulacji,
tak że "nie byłoby więcej potrzeby sporu między dwoma filozofami niż między dwoma księgowymi, ponieważ wystarczyłoby wziąć
ich ołówki ręcznie, w dół do swoich plansz i powiedzieć sobie nawzajem (z przyjacielem jako świadkiem, jeśli im się podobało):
Obliczmy . " Ci filozofowie zaczęli wyartykułować hipotezę fizycznego systemu symboli, która stanie się wiodącą wiarą w
badania nad sztuczną inteligencją.

W XX wieku badania nad logiką matematyczną dostarczyły przełomu, który sprawił, że sztuczna inteligencja stała się prawdopodobna.
 Podstawy zostały ustalone przez takie dzieła, jak Boole 's The Laws of Thought and Frege 's Begriffsschrift . Opierając się na
 systemie Fregego , Russell i Whitehead przedstawili formalne potraktowanie podstaw matematyki w swoich arcydziełach, Principia
  Mathematica w 1913 roku. Zainspirowany sukcesem Russella , David Hilbert zakwestionował matematyków z lat dwudziestych i
  trzydziestych, aby odpowiedzieć na to zasadnicze pytanie : "Czy całe matematyczne rozumowanie można sformalizować?"
  Na jego pytanie odpowiedział dowód Gödla na niezupełność , maszynę Turinga i rachunek Lambda Kościoła .
ENIAC w Moore School of Electrical Engineering. To zdjęcie zostało sztucznie przyciemnione, zaciemniając szczegóły, takie jak
kobiety, które były obecne i używany sprzęt IBM.

Ich odpowiedź była zaskakująca na dwa sposoby. Po pierwsze udowodnili, że faktycznie istnieją granice tego, co może osiągnąć
logika matematyczna. Ale po drugie (i ważniejsze dla AI) ich praca sugerowała, że ​​w tych granicach można zmechanizować wszelkie
formy matematycznego rozumowania. Teza Churching-Turinga sugerowała, że ​​mechaniczne urządzenie, przesuwające symbole tak proste
jak 0 i 1, może naśladować każdy możliwy do wyobrażenia proces matematycznego wnioskowania. Kluczowym wglądem była maszyna
Turinga - prosta teoretyczna konstrukcja, która uchwyciła istotę manipulacji abstrakcyjnym symbolem. Ten wynalazek zainspirowałby
garstkę naukowców do rozpoczęcia dyskusji na temat możliwości myślenia maszyn.
    </div>
    <div class="">
      <h2>Informatyka</h2>

      Maszyny liczące zostały zbudowane w starożytności i ulepszone przez historię wielu matematyków, w tym (po raz kolejny)
      filozofa Gottfrieda Leibniza . Na początku XIX wieku Charles Babbage zaprojektował programowalny komputer ( silnik
      analityczny ), chociaż nigdy go nie zbudowano. Ada Lovelace spekulowała, że ​​maszyna "może komponować wyszukane i naukowe
      utwory o dowolnym stopniu złożoności lub zasięgu". (Jest ona często uznawana za pierwszego programistę ze względu
      na zestaw notatek, które napisała, szczegółowo opisującą metodę obliczania liczb Bernoulliego za pomocą silnika).

      Pierwszymi nowoczesnymi komputerami były masywne maszyny kodujące II wojnę światową (takie jak Z3 , ENIAC i Colossus ).
       Dwie ostatnie z tych maszyn oparte były na teoretycznej podstawie Alana Turinga i opracowanej przez Johna von
       Neumanna.
    </div>
    <div class="">
      <h2>Narodziny sztucznej inteligencji</h2>

      W latach 40. i 50. garstka naukowców z różnych dziedzin (matematyka, psychologia, inżynieria, ekonomia i politologia)
      zaczęła dyskutować o możliwości stworzenia sztucznego mózgu. Dziedzina badań nad sztuczną inteligencją została założona
      jako dyscyplina akademicka w 1956 roku.
    </div>
    <div class="">
      <h2>Cybernetyka i wczesne sieci neuronowe</h2>
      Najwcześniejsze badania nad myślącymi maszynami zostały zainspirowane zbieżnością idei, która stała się rozpowszechniona
      w późnych latach trzydziestych, w latach 40. i na początku lat pięćdziesiątych. Ostatnie badania neurologiczne wykazały,
      że mózg był siecią elektryczną neuronów, które wystrzeliwały w impulsy typu "wszystko albo nic". Cybernetyka Norberta
      Wienera opisuje kontrolę i stabilność w sieciach elektrycznych. Teoria informacji Claude'a Shannona opisywała sygnały
      cyfrowe (tj. Sygnały "wszystko albo nic"). Teoria obliczeń Alana Turinga pokazała, że ​​dowolna forma obliczeń może być
      opisana cyfrowo. Bliska relacja między tymi pomysłami sugerowała, że ​​możliwe jest skonstruowanie elektronicznego mózgu .


Przykłady prac w tym duchu obejmują roboty, takie jak żółwie W. Greya Waltera i bestia z Johns Hopkins . Maszyny te nie używały
komputerów, cyfrowej elektroniki ani symbolicznego rozumowania; były całkowicie kontrolowane przez obwody analogowe.

Walter Pitts i Warren McCulloch przeanalizowali sieci wyidealizowanych sztucznych neuronów i pokazali, w jaki sposób mogą
wykonywać proste funkcje logiczne. Byli pierwszymi, którzy opisali to, co później naukowcy nazwaliby siecią neuronową. Jednym
z uczniów zainspirowanych przez Pittsa i McCullocha był młody Marvin Minsky , a następnie 24-letni absolwent. W 1951 r.
(Wraz z Deanem Edmondsem) zbudował pierwszą sieć neuronową SNARC. Minsky miał zostać jednym z najważniejszych liderów i
innowatorów w AI przez następne 50 lat.
    </div>
    <div class="">
      <h2>Test Turinga</h2>
      W 1950 roku Alan Turing opublikował przełomowy artykuł, w którym spekulował na temat możliwości tworzenia maszyn, które
      myślą. Zauważył, że "myślenie" jest trudne do zdefiniowania i wymyślił jego słynny test Turinga . Jeśli maszyna
      mogłaby prowadzić rozmowę (przez teleprintera ), która byłaby nieodróżnialna od rozmowy z człowiekiem, wtedy rozsądnym
      było powiedzieć, że maszyna "myślała". Ta uproszczona wersja problemu pozwoliła Turingowi przekonująco argumentować, że
      "maszyna myśląca" była co najmniej wiarygodna, a artykuł odpowiedział na wszystkie najczęstsze zastrzeżenia do tej
      propozycji. Test Turinga był pierwszą poważną propozycją w filozofii sztucznej inteligencji .
    </div>
    <div class="">
      <h2>Gra AI</h2>
      W 1951 roku, używając maszyny Ferranti Mark 1 na Uniwersytecie w Manchesterze , Christopher Strachey napisał program
      sprawdzający, a Dietrich Prinz napisał jeden do szachów. Program warcabów Arthura Samuela , opracowany w połowie
      lat 50. i wczesnych 60., ostatecznie osiągnął wystarczające umiejętności, aby rzucić wyzwanie szanowanemu amatorowi.
      Gra sztucznej inteligencji byłaby nadal wykorzystywana jako miara postępu w sztucznej inteligencji w całej jej historii.
    </div>
    <div class="">
      <h2>Symboliczne rozumowanie i teoria logiki</h2>
      Gdy dostęp do komputerów cyfrowych stał się możliwy w połowie lat pięćdziesiątych, kilku naukowców instynktownie uznało,
      że maszyna, która może manipulować liczbami, może również manipulować symbolami i że manipulacja symbolami może być
      istotą ludzkiej myśli. To było nowe podejście do tworzenia maszyn myślących.

W 1955 roku Allen Newell i (przyszły laureat Nagrody Nobla) Herbert A. Simon stworzył " Teoretyka logiki " (z pomocą JC Shawa ).
Program ostatecznie dowiedzie 38 z pierwszych 52 twierdzeń w Principia Mathematica Russella i Whiteheada i znajdzie dla
niektórych nowe i bardziej eleganckie dowody. Szymon powiedział, że "rozwiązali czcigodny problem z ciałem / umysłem ,
wyjaśniając, jak system złożony z materii może mieć właściwości umysłu". (Było to wczesne stwierdzenie stanowiska
filozoficznego, które John Searle nazwał później " Silną sztuczną inteligencją ": maszyny mogą zawierać umysły tak jak
ludzkie ciała).
    </div>
    <div class="">
      <h2>Dartmouth Conference 1956: narodziny AI</h2>

      Konferencja w Dartmouth z 1956 r. została zorganizowana przez Marvina Minsky'ego , Johna McCarthy'ego i dwóch
      starszych naukowców: Claude'a Shannona i Nathana Rochestera z IBM . Propozycja konferencji zawierała następujące
      stwierdzenie: "każdy aspekt uczenia się lub jakakolwiek inna cecha inteligencji może być tak dokładnie opisana, że
      ​​można stworzyć maszynę symulującą". Wśród uczestników znaleźli się Ray Solomonoff , Oliver Selfridge , Trenchard
      More , Arthur Samuel , Allen Newell i Herbert A. Simon , którzy stworzyli ważne programy podczas pierwszych
      dziesięcioleci badań nad sztuczną inteligencją. Na konferencji Newell i Simon zadebiutowali " Teoretykiem logiki ",
       a McCarthy namówił uczestników, aby przyjęli "Sztuczną inteligencję" jako nazwę pola. Konferencja w Dartmouth
       w 1956 r. Była momentem, kiedy AI zdobyło swoją nazwę, swoją misję, swój pierwszy sukces i głównych graczy, i jest
       powszechnie uważane za narodziny sztucznej inteligencji.
    </div>
    <div class="">
      <h2>Złote lata 1956-1974</h2>

      Lata po konferencji w Dartmouth były erą odkryć, sprintu na nowym gruncie. Programy opracowane w tym czasie dla
      większości ludzi były po prostu "zadziwiające": komputery rozwiązywały problemy z słowem algebry, dowodząc
      twierdzeń w geometrii i ucząc się mówić po angielsku. W owym czasie niewielu wierzyłoby, że takie "inteligentne"
      zachowanie się maszyn jest w ogóle możliwe. Badacze wyrazili duży optymizm zarówno w sferze prywatnej, jak i
      drukarskiej, przewidując, że w pełni inteligentna maszyna zostanie zbudowana w czasie krótszym niż 20 lat.
      Agencje rządowe takie jak DARPA wylewały pieniądze na nowe pole.


      Było wiele udanych programów i nowych kierunków w późnych latach 50. i 60. XX wieku. Do najbardziej wpływowych należą:
Rozumowanie jako wyszukiwanie

Wiele wczesnych programów sztucznej inteligencji używało tego samego podstawowego algorytmu . Aby osiągnąć jakiś cel (np. Wygrać
 grę lub udowodnić twierdzenie), postępowali krok po kroku w tym kierunku (wykonując ruch lub dedukcję), jakby szukając w
 labiryncie, wycofując się za każdym razem, gdy osiągnęli ślepy zaułek. Ten paradygmat nazwano " rozumowaniem jako przeszukaniem
 ".

Główna trudność polegała na tym, że dla wielu problemów liczba możliwych ścieżek przez "labirynt" była po prostu astronomiczna
(sytuacja określana jako " kombinatoryczna eksplozja "). Naukowcy zmniejszyliby przestrzeń poszukiwań za pomocą heurystyki lub
" reguł kciuka ", które wyeliminowałyby te ścieżki, które raczej nie doprowadziłyby do rozwiązania.

Newell i Simon próbowali uchwycić ogólną wersję tego algorytmu w programie o nazwie " General Problem Solver ". Inne
 programy "poszukujące" były w stanie wykonać imponujące zadania, takie jak rozwiązywanie problemów z geometrii i algebry,
 takie jak Geometry Theorem Prover Herberta Gelerntera (1958) i SAINT , napisane przez studenta Minsky'ego James Slagle (1961).
 Inne programy przeszukiwały cele i podziały, aby zaplanować działania, takie jak system STRIPS opracowany w Stanford w
  celu kontrolowania zachowania ich robota Shakey.
Przykład sieci semantycznej
Język naturalny

Ważnym celem badań nad sztuczną inteligencją jest umożliwienie komputerom komunikowania się w językach naturalnych, takich
jak angielski. Szybkim sukcesem był program STUDENT Daniela Bobrowa , który mógł rozwiązać problemy ze słowem algebry
licealnej.

Sieć semantyczna reprezentuje pojęcia (np. "Dom", "drzwi") jako węzły i relacje między pojęciami (np. "Has-a") jako połączenia
między węzłami. Pierwszy program AI do korzystania z sieci semantycznej został napisany przez Rossa Quilliana, a
najbardziej udaną (i kontrowersyjną) wersją była koncepcyjna koncepcja zależności Rogera Schanka.

ELIZA Josepha Weizenbauma mogła prowadzić rozmowy tak realistyczne, że użytkownicy czasami byli oszukiwani, myśląc, że
komunikują się z człowiekiem, a nie z programem. W rzeczywistości jednak ELIZA nie miała pojęcia, o czym mówi. Po prostu
 podała odpowiedź z puszki lub powtórzyła to, co jej powiedziano, zmieniając jej odpowiedź za pomocą kilku zasad gramatyki.
 ELIZA była pierwszą chatterbotą
 .
Mikroświaty

Pod koniec lat 60. Marvin Minsky i Seymour Papert z Laboratorium MIT AI zaproponowali, aby badania nad sztuczną inteligencją
koncentrowały się na sztucznie prostych sytuacjach zwanych mikro-światami. Wskazywali oni, że w naukach odnoszących sukces,
 takich jak fizyka, podstawowe zasady były często najlepiej rozumiane za pomocą uproszczonych modeli, takich jak bezcieniowe
  płaszczyzny lub idealnie sztywne ciała. Większość badań skupiała się na " świecie bloków ", który składa się z kolorowych
  bloków o różnych kształtach i rozmiarach ułożonych na płaskiej powierzchni.

Ten paradygmat doprowadził do nowatorskiej pracy w zakresie widzenia maszynowego przez Geralda Sussmana
(który kierował zespołem), Adolfa Guzmana , Davida Waltza (który wynalazł " propagację ograniczeń "), a zwłaszcza Patricka
Winstona . W tym samym czasie Minsky i Papert zbudowali ramię robota, które może układać klocki, wprowadzając bloki w życie.
 Ukoronowaniem programu mikro-świata była SHRDLU Terry'ego Winograda . Może komunikować się w zwykłych angielskich zdaniach,
  planować operacje i wykonywać je.
    </div>
    <div class="">
      <h2>Optymizm</h2>

      Pierwsza generacja badaczy sztucznej inteligencji opracowała te prognozy dotyczące ich pracy:

    1958, HA Simon i Allen Newell : "w ciągu dziesięciu lat komputer cyfrowy stanie się mistrzem świata w szachach" i "w ciągu
    dziesięciu lat komputer cyfrowy odkryje i udowodni ważne nowe matematyczne twierdzenie".
    1965, HA Simon : "maszyny będą zdolne, w ciągu dwudziestu lat, wykonać jakąkolwiek pracę, którą człowiek może zrobić."
    1967, Marvin Minsky : "W ciągu pokolenia ... problem tworzenia" sztucznej inteligencji "zostanie zasadniczo rozwiązany."
    1970, Marvin Minsky (w Life Magazine ): "Za trzy do ośmiu lat będziemy dysponować maszyną z ogólną inteligencją przeciętnego
     człowieka".
    </div>
    <div class="">
      <h2>Pieniądze</h2>

      W czerwcu 1963 r. MIT otrzymał dotację w wysokości 2,2 miliona USD od nowo utworzonej Agencji Zaawansowanych Projektów
      Badawczych (później znanej jako DARPA ). Pieniądze zostały wykorzystane do sfinansowania projektu MAC, który został
      podporządkowany "Grupie AI" założonej przez Minsky'ego i McCarthy'ego pięć lat wcześniej. DARPA nadal dostarczała trzy
      miliony dolarów rocznie do lat 70-tych. DARPA udzielił podobnych dotacji programowi Newella i Simona w CMU oraz w
      projekcie Stanford AI (założonym przez Johna McCarthy'ego w 1963 r.). Kolejne ważne laboratorium AI zostało założone
      na Uniwersytecie w Edynburgu przez Donalda Michiego w 1965 r. Te cztery instytucje będą przez wiele lat głównym
      ośrodkiem badań nad sztuczną inteligencją (i finansowaniem) w środowisku akademickim.

Pieniądze zostały przekazane w kilku załącznikach: JCR Licklider , a następnie dyrektor ARPA , uważał, że jego organizacja
powinna "finansować ludzi, a nie projekty!" i pozwoliło badaczom realizować dowolne kierunki, które mogą ich zainteresować. Stworzyło to swobodną atmosferę w MIT, która zrodziła kulturę hakerską, ale to podejście "odkładające ręce" nie
potrwa długo.
    </div>
    <div class="">
      <h2>Robotyka</h2>

      W Japonii Uniwersytet Waseda zainicjował projekt WABOT w 1967 roku, aw 1972 roku ukończył WABOT-1, pierwszy na świecie
      inteligentny robot humanoidalny na pełną skalę, lub android . System kontroli kończyny pozwalał mu chodzić z
      kończynami dolnymi oraz chwytać i transportować przedmioty rękami, używając czujników dotykowych. System wizyjny pozwalał
       mu mierzyć odległości i kierunki do obiektów za pomocą zewnętrznych receptorów, sztucznych oczu i uszu. A system
       konwersacji pozwalał mu komunikować się z osobą po japońsku, ze sztucznymi ustami
    </div>
    <div class="">
      <h2>Spowolnienie badań 1974-1980 </h2>

      W latach 70. AI była przedmiotem krytyki i finansowych niepowodzeń. Naukowcy z AI nie byli w stanie docenić trudności,
      z jakimi mieli do czynienia. Ich ogromny optymizm podniósł oczekiwania niemożliwie wysokie, a gdy obiecane rezultaty
      nie doszły do ​​skutku, zniknęły fundusze na sztuczną inteligencję. W tym samym czasie pole połączenia (lub sieci
      neuronowe ) zostało prawie całkowicie wyłączone na 10 lat przez dewastującą krytykę perceptronów Marvina Minsky'ego .
      Pomimo trudności z publicznym postrzeganiem sztucznej inteligencji w późnych latach 70., odkryto nowe pomysły w
      programowaniu logiki , zdroworozsądkowym rozumowaniu i wielu innych dziedzinach.
    </div>
    <div class="">
      <h2>Problemy</h2>

      Na początku lat siedemdziesiątych możliwości programów sztucznej inteligencji były ograniczone. Nawet najbardziej
      imponujące potrafiły poradzić sobie jedynie z trywialnymi wersjami problemów, które miały rozwiązać; wszystkie programy
      były w pewnym sensie "zabawkami". Naukowcy zajmujący się sztuczną inteligencją zaczęli napotykać szereg podstawowych
      ograniczeń, których nie można było pokonać w latach 70. XX wieku. Chociaż niektóre z tych limitów zostaną podbite w
      późniejszych dziesięcioleciach, inne jeszcze nadal utorują drogę do dnia dzisiejszego.

    Ograniczona moc komputera : brakowało pamięci lub szybkości przetwarzania, aby osiągnąć wszystko, co naprawdę przydatne.
    Na przykład udana praca Rossa Quilliana nad językiem naturalnym została zademonstrowana za pomocą słownika składającego
    się z zaledwie dwudziestu słów, ponieważ to wszystko pasowało do pamięci. Hans Moravec twierdził w 1976 r., Że
    komputery są nadal miliony razy za słabe, by pokazać inteligencję. Zaproponował analogię: sztuczna inteligencja wymaga
    mocy komputera w taki sam sposób, w jaki samolot potrzebuje mocy . Poniżej pewnego progu jest to niemożliwe, ale w miarę
    wzrostu mocy może się to w końcu okazać łatwe. Jeśli chodzi o wizję komputerową, Moravec oszacował, że proste
    dopasowanie możliwości krtani i wykrywania ruchu ludzkiej siatkówki w czasie rzeczywistym wymagałoby komputera ogólnego
    przeznaczenia zdolnego do 10 operacji na sekundę (1000 MIPS). Od 2011 r. Praktyczne zastosowania w komputerowej
    wizji wymagają od 10 000 do 1 000 000 MIPS. Dla porównania, najszybszy superkomputer w 1976 roku, Cray-1 (sprzedaż
    detaliczna od 5 milionów do 8 milionów USD), był w stanie osiągnąć tylko od 80 do 130 MIPS, a typowy komputer
    stacjonarny w tym czasie osiągnął mniej niż 1 MIPS.
    Nietrwałość i kombinatoryczna eksplozja . W 1972 r. Richard Karp (opierając się na twierdzeniu Stephena Cooka z 1971 r.)
    Wykazał, że istnieje wiele problemów, które prawdopodobnie można rozwiązać tylko w czasie wykładniczym (w rozmiarze
    danych wejściowych). Znalezienie optymalnych rozwiązań tych problemów wymaga niewyobrażalnych ilości czasu komputera,
    z wyjątkiem sytuacji, gdy problemy są banalne. To prawie na pewno oznaczało, że wiele "zabawkowych" rozwiązań
    wykorzystywanych przez sztuczną inteligencję prawdopodobnie nigdy nie skaluje się w użyteczne systemy.
    Zdrowa wiedza i rozumowanie . Wiele ważnych zastosowań sztucznej inteligencji, takich jak wizja lub język naturalny,
    wymaga po prostu ogromnej ilości informacji o świecie: program musi mieć pewne wyobrażenie o tym, na co patrzy lub o
    czym mówi. Wymaga to, aby program znał te same rzeczy na temat świata, co dziecko. Badacze szybko odkryli, że jest to
    naprawdę ogromna ilość informacji. Nikt w 1970 roku nie był w stanie zbudować tak dużej bazy danych i nikt nie wiedział,
    jak program może nauczyć się tak wielu informacji.
    Paradoks Moraveca : Udowodnienie twierdzeń i rozwiązywanie problemów geometrii jest stosunkowo łatwe dla komputerów, ale
    rzekomo proste zadanie, takie jak rozpoznawanie twarzy lub przekraczanie pomieszczenia bez wpadania na nic, jest niezwykle
    trudne. Pomaga to wyjaśnić, dlaczego badania w zakresie wizji i robotyki uczyniły tak niewielki postęp w połowie lat
    siedemdziesiątych.
    Problemy z ramą i kwalifikacjami . Naukowcy zajmujący się sztuczną inteligencją (tacy jak John McCarthy ), którzy stosowali
    logikę, odkryli, że nie mogą reprezentować zwykłych dedukcji związanych z planowaniem lub domyślnym rozumowaniem bez
     wprowadzania zmian w samej strukturze logiki. Opracowali nowe logiki (takie jak niemonotoniczne logiki i logiki modalne ),
     aby spróbować rozwiązać problemy.
    </div>
    <div class="">
      <h2>Koniec finansowania</h2>

      Agencje finansujące badania nad sztuczną inteligencją (takie jak brytyjski rząd , DARPA i NRC ) zostały sfrustrowane
      brakiem postępów i ostatecznie zniosły prawie wszystkie fundusze na nie ukierunkowane badania nad sztuczną inteligencją.
       Wzór rozpoczął się już w 1966 r., Kiedy pojawił się raport ALPAC krytykujący wysiłki związane z tłumaczeniem maszynowym.
        Po wydaniu 20 milionów dolarów NRC zakończyło wszelkie wsparcie. W 1973 r. Raport Lighthilla na temat stanu badań
        nad sztuczną inteligencją w Anglii skrytykował całkowitą porażkę AI w osiągnięciu swoich "wspaniałych celów" i
        doprowadził do zlikwidowania badań nad sztuczną inteligencją w tym kraju. (W raporcie wspomniano konkretnie o
         problemie wybuchu kombinatorycznego jako przyczynie niedociągnięć AI.) DARPA była głęboko rozczarowana badaczami
         pracującymi nad programem badania rozumienia mowy w CMU i anulowała roczną dotację w wysokości trzech milionów
         dolarów. W 1974 roku trudno było znaleźć fundusze na projekty sztucznej inteligencji.

Hans Moravec obwinił kryzys o nierealistyczne przewidywania swoich kolegów. "Wielu badaczy uwikłało się w sieć coraz większej
przesady." Pojawiła się jednak inna kwestia: od czasu wprowadzenia poprawki Mansfield w 1969 r. DARPA znajdowała się pod
coraz większą presją, by sfinansować "bezpośrednie badania ukierunkowane na misję, a nie podstawowe, niekierowane badania".
Finansowanie twórczej, swobodnej eksploracji, która miała miejsce w latach 60., nie pochodzi od DARPA . Zamiast tego pieniądze
zostały przeznaczone na konkretne projekty o jasnych celach, takich jak autonomiczne czołgi i systemy zarządzania bitwą.
    </div>
    <div class="">
      <h2>Fala krytyki</h2>

      Kilku filozofów miało poważne zastrzeżenia do twierdzeń wysuwanych przez badaczy sztucznej inteligencji. Jednym z
      pierwszych był John Lucas , który twierdził, że twierdzenie Gödla o niezupełności pokazuje, że formalny system (taki
      jak program komputerowy) nigdy nie byłby w stanie zobaczyć prawdy pewnych wypowiedzi, podczas gdy człowiek mógł.
      Hubert Dreyfus wyśmiewał złamane obietnice z lat 60. i krytykował założenia SI, argumentując, że ludzkie rozumowanie w
      rzeczywistości zawierało bardzo niewiele "przetwarzania symboli" i wiele wcielonego , instynktownego , nieświadomego "
      know how ". Argument Johna Searle'a w sprawie chińskiego pokoju , przedstawiony w 1980 roku, próbował pokazać, że
      nie można powiedzieć, że program "rozumie" symbole, których używa (jakość określana jako " intencjonalność "). Jeśli
      symbole nie mają znaczenia dla maszyny, argumentował Searle, to maszyna nie może być opisana jako "myśląca".

Krytyki te nie zostały poważnie potraktowane przez badaczy sztucznej inteligencji, często dlatego, że wydawały się tak odległe.
 Problemy takie jak trudność i zdroworozsądkowa wiedza wydawały się o wiele bardziej naglące i poważne. Nie było jasne, jaka
 różnica " wiedzieć jak " lub " intencjonalność " została wprowadzona do rzeczywistego programu komputerowego. Minsky
  powiedział o Dreyfusie i Searle "źle rozumieją i należy je zignorować". [96] Dreyfus, który uczył w MIT , dostał chłodnego
  ramienia: później powiedział, że naukowcy z AI "nie ośmielili się widzieć, gdy jedzą ze mną lunch". Joseph Weizenbaum ,
   autor ELIZA , czuł, że traktowanie Dreyfusa przez jego kolegów było nieprofesjonalne i dziecinne. Chociaż był zdeklarowanym
    krytykiem pozycji Dreyfusa, "celowo dał do zrozumienia, że ​​to nie jest sposób na traktowanie człowieka".

Weizenbaum zaczął mieć poważne wątpliwości etyczne dotyczące AI, gdy Kenneth Colby napisał "program komputerowy, który może
prowadzić dialog psychoterapeutyczny" oparty na ELIZA. Weizenbaum był zaniepokojony tym, że Colby widział bezmyślny
program jako poważne narzędzie terapeutyczne. Rozpoczął się feud, a sytuacja nie pomogła, gdy Colby nie uznał Weizenbaum
za jego wkład w program. W 1976 roku Weizenbaum opublikował Computer Power and Human Reason, które dowodziły, że nadużywanie
sztucznej inteligencji ma potencjał do dewaluacji ludzkiego życia.
    </div>
    <div class="">
      <h2>Perceptrony</h2>

      Perceptron był formą sieci neuronowej wprowadzonej w 1958 roku przez Franka Rosenblatta , który był szkolnym kolegą
      Marvina Minsky'ego z Bronx High School of Science . Podobnie jak większość badaczy AI, był optymistą co do ich mocy,
      przewidując, że "perceptron może ostatecznie być w stanie uczyć się, podejmować decyzje i tłumaczyć języki". Aktywny
      program badawczy dotyczący paradygmatu przeprowadzono w latach sześćdziesiątych, ale nagle zatrzymał się on wraz z
      opublikowaniem książki Perceptrons Minsky'ego i Paperta z 1969 roku. Sugerowało to, że istnieją poważne ograniczenia
      perceptronu, a prognozy Franka Rosenblatta zostały znacznie przesadzone. Efekt książki był druzgocący: praktycznie
      nie przeprowadzono żadnych badań w związku z połączeniami przez 10 lat. Ostatecznie nowe pokolenie badaczy ożywiłoby
       tę dziedzinę, a następnie stałoby się istotną i przydatną częścią sztucznej inteligencji. Rosenblatt nie doczekałby
       tego, ponieważ zmarł w wypadku łodzi krótko po opublikowaniu książki
    </div>
    <div class="">
      <h2>Logika i rozumowanie symboliczne</h2>

      Logika została wprowadzona do badań nad sztuczną inteligencją już w 1958 r. Przez Johna McCarthy'ego w jego propozycjach "
      Advice Taker". W 1963 r. J. Alan Robinson odkrył prostą metodę implementacji dedukcji na komputerach,
      rozdzielczości i algorytmu unifikacji . Jednak proste implementacje, takie jak próby McCarthy'ego i jego uczniów pod
      koniec lat 60., były szczególnie trudne: programy wymagały astronomicznej liczby kroków, aby udowodnić proste
      twierdzenia. Bardziej owocne podejście do logiki opracował w latach siedemdziesiątych Robert Kowalski z
      Uniwersytetu w Edynburgu i wkrótce doprowadziło to do współpracy z francuskimi badaczami Alainem Colmerauerem i
      Philippe Roussel, którzy stworzyli udany język programowania logicznego Prolog . Prolog używa podzbioru
      logiki ( klauzul Horn , ściśle związanych z "regułami" i " regułami produkcji "), które pozwalają na łatwe obliczenia.
      Reguły nadal będą wpływać, zapewniając fundamenty dla systemów eksperckich Edwarda Feigenbauma i kontynuację pracy
       Allena Newella i Herberta A. Simona , które doprowadzą do Soaru i ich zjednoczonych teorii poznania.

Krytycy logicznego podejścia zauważyli, jak to ujął Dreyfus , że ludzie rzadko stosowali logikę, gdy rozwiązywali problemy.
Eksperymenty przeprowadzone przez psychologów takich jak Peter Wason , Eleanor Rosch , Amos Tversky , Daniel Kahneman i inni
 dostarczyli dowodów. McCarthy odpowiedział, że to, co robią ludzie, jest nieistotne. Twierdził, że tak naprawdę
  potrzebne są maszyny, które mogą rozwiązywać problemy, a nie maszyny, które myślą tak, jak ludzie.
    </div>
    <div class="">
      <h2>Scruffies: frames and scripts</h2>

      Wśród krytyków podejścia McCarthy'ego byli jego koledzy z całego kraju w MIT . Marvin Minsky , Seymour Papert i
      Roger Schank próbowali rozwiązać problemy takie jak "zrozumienie fabuły" i "rozpoznawanie obiektów", które wymagały
       od maszyny myślenia jak osoby. Aby używać zwykłych pojęć, takich jak "krzesło" lub "restauracja", musieli oni robić
        te same nielogiczne założenia, które ludzie zwykle robili. Niestety, takie nieprecyzyjne pojęcia są trudne do
         przedstawienia w logice. Gerald Sussman zauważył, że "używanie precyzyjnego języka do opisywania w zasadzie
         nieprecyzyjnych pojęć nie czyni ich bardziej precyzyjnymi". Schank określił ich podejście " antylogiczne "
         jako " niechlujne ", w przeciwieństwie do " czystych " paradygmatów używanych przez McCarthy'ego , Kowalskiego ,
          Feigenbauma , Newella i Simona.

W 1975 roku, w przełomowym artykule, Minsky zauważył, że wielu jego kolegów "niechlujnych" badaczy używa tego samego
narzędzia: ramy, które wychwytuje wszystkie nasze zdroworozsądkowe założenia o czymś. Na przykład, jeśli użyjemy pojęcia
 ptaka, pojawia się konstelacja faktów, które natychmiast przychodzą nam do głowy: możemy założyć, że leci, zjada robaki
  i tak dalej. Wiemy, że fakty te nie zawsze są prawdziwe i że odliczenia z wykorzystaniem tych faktów nie będą "logiczne",
   ale te uporządkowane zestawy założeń są częścią kontekstu wszystkiego, co mówimy i myślimy. Nazwał te struktury
    " ramami ". Schank użył wersji ramek, które nazwał " skryptami ", aby skutecznie odpowiadać na pytania o opowiadania
    w języku angielskim. Wiele lat później programowanie obiektowe przyswoiłoby podstawową ideę " dziedziczenia "
    z badań AI nad ramami.
    </div>
    <div class="">
      <h2>Bum 1980-1987</h2>

      W latach 80. XX wieku korporacje na całym świecie przyjęły formę programu o nazwie " systemy eksperckie ", a
      wiedza stała się przedmiotem głównego nurtu badań nad sztuczną inteligencją. W tych samych latach rząd japoński
      agresywnie finansował AI z projektem komputerowym piątej generacji . Innym zachęcającym wydarzeniem we wczesnych
      latach 80. było ożywienie związku w twórczości Johna Hopfielda i Davida Rumelharta . Po raz kolejny AI osiągnęła
       sukces.
Powstanie systemów ekspertowych

System ekspercki to program, który odpowiada na pytania lub rozwiązuje problemy dotyczące określonej dziedziny wiedzy,
stosując zasady logiczne , które wywodzą się z wiedzy ekspertów. Najwcześniejsze przykłady zostały opracowane przez
Edwarda Feigenbauma i jego uczniów. Dendral , rozpoczęty w 1965 roku, zidentyfikował związki z odczytów spektrometru.
MYCIN , opracowany w 1972 roku, zdiagnozował zakaźne choroby krwi. Wykazali oni wykonalność podejścia.

Systemy ekspertowe ograniczyły się do małej domeny o określonej wiedzy (dzięki czemu uniknęliśmy problemu wiedzy teoretycznej ),
 a ich prosta konstrukcja sprawiła, że ​​stosunkowo łatwo można było budować programy, a następnie modyfikować je po ich
 wprowadzeniu. W sumie programy okazały się przydatne : coś, czego AI nie udało się osiągnąć do tego momentu.

W 1980 roku system ekspertowy o nazwie XCON został ukończony w CMU dla Digital Equipment Corporation . Był to ogromny sukces:
 firma oszczędzała firmie 40 milionów dolarów rocznie w 1986 r. Korporacje na całym świecie zaczęły opracowywać i
  wdrażać systemy eksperckie, a do 1985 r. Wydały ponad miliard dolarów na sztuczną inteligencję, z czego większość na
  Domowe działy AI. Rozwijał się przemysł, aby je wspierać, w tym firmy sprzętowe, takie jak Symbolics i Lisp Machines oraz
  firmy programistyczne, takie jak IntelliCorp i Aion.
Rewolucja wiedzy

Potęga systemów eksperckich pochodziła z wiedzy eksperckiej, którą one zawierały. Stanowiły one część nowego kierunku badań
nad sztuczną inteligencją, które zyskały popularność w latach 70-tych. "Naukowcy z AI zaczęli podejrzewać - niechętnie,
ponieważ naruszyło to naukowy kanon parsimony - że inteligencja może być bardzo dobrze oparta na zdolności wykorzystywania
 dużej ilości różnorodnej wiedzy na różne sposoby", pisze Pamela McCorduck . "Wspaniałą lekcją z lat 70. było to, że
 inteligentne zachowanie zależało w dużym stopniu od radzenia sobie z wiedzą, czasem dość szczegółową wiedzą, z domeny, w
  której określone jest zadanie". Systemy oparte na wiedzy i inżynieria wiedzy stały się głównym przedmiotem badań
  nad sztuczną inteligencją w latach 80.

W latach osiemdziesiątych narodził się Cyc , pierwsza próba bezpośredniego ataku na problem wiedzy ogólnej , poprzez stworzenie
ogromnej bazy danych, która zawierałaby wszystkie przyziemne fakty, które zna przeciętny człowiek. Douglas Lenat , który
rozpoczął i kierował projektem, argumentował, że nie ma skrótu - jedynym sposobem, aby maszyny poznały znaczenie ludzkich
 pojęć, jest nauczanie ich, jednej koncepcji na raz, ręcznie. Projekt nie miał się zakończyć od wielu dziesięcioleci.

Programy szachowe HiTech i Deep Thought pokonały szachistów w 1989 roku. Oba zostały opracowane przez Carnegie Mellon University;
 Projekt Deep Thought utorował drogę dla Deep Blue.
Zwrot pieniędzy: projekt piątej generacji

W 1981 r. Japońskie Ministerstwo Handlu Międzynarodowego i Przemysłu odłożyło 850 milionów dolarów na projekt komputerowy piątej
 generacji . Ich celem było pisanie programów i budowanie maszyn, które mogłyby prowadzić rozmowy, tłumaczyć języki,
  interpretować obrazy i rozumować jak ludzie. Ku rozgoryczeniu zbrodni , wybrali Prolog jako główny język komputerowy
  projektu.

Inne kraje odpowiedziały nowymi programami. Wielka Brytania rozpoczęła ₤ 350 milionów projektów Alvey . Konsorcjum
amerykańskich firm utworzyło Microelectronics and Computer Technology Corporation (lub "MCC"), aby sfinansować duże
 projekty w zakresie sztucznej inteligencji i technologii informacyjnej. DARPA również zareagowała,
 tworząc strategiczną inicjatywę obliczeniową i potrajając swoje inwestycje w sztuczną inteligencję w latach 1984-1988.
Sieć Hopfield z czterema węzłami.
Odrodzenie związku

W 1982 roku fizyk John Hopfield był w stanie udowodnić, że forma sieci neuronowej (obecnie nazywanej " siecią Hopfielda ")
 może uczyć się i przetwarzać informacje w zupełnie nowy sposób. Mniej więcej w tym samym czasie David Rumelhart
 spopularyzował nową metodę szkolenia sieci neuronowych zwanych " backpropagation " (odkryta wiele lat wcześniej
 przez Paula Werbosa ). Te dwa odkrycia ożywiły pole połączenia, które zostało w dużej mierze porzucone od 1970
 roku.

Nowe pole zostało zunifikowane i zainspirowane pojawieniem się Parallel Distributed Processing w 1986 roku - dwóch
tomów kolekcji opracowanych przez Rumelharta i psychologa Jamesa McClellanda . Sieci neuronowe odniosłyby komercyjny
sukces w latach 90., kiedy zaczęły być wykorzystywane jako silniki napędzające programy, takie jak optyczne rozpoznawanie
znaków i rozpoznawanie mowy.
    </div>
    <div class="">
      <h2>Spowolnienie badań nad AI 1987-1993</h2>

      Fascynacja AI społeczności biznesowej wzrosła i spadła w latach 80. w klasycznym modelu bańki gospodarczej .
      Załamanie polegało na postrzeganiu sztucznej inteligencji przez agencje rządowe i inwestorów - mimo krytyki
      nadal poczyniła postępy. Rodney Brooks i Hans Moravec , naukowcy z pokrewnej dziedziny robotyki , argumentowali
       za całkowicie nowym podejściem do sztucznej inteligencji.
AI winter

Termin " sztuczna zima " został wymyślony przez badaczy, którzy przetrwali cięcia w finansach w 1974 r., Kiedy obawiali
się, że entuzjazm dla systemów eksperckich wymknął się spod kontroli i że z pewnością nastąpi rozczarowanie. Ich
obawy były uzasadnione: pod koniec lat 80. i na początku lat 90. AI poniosła szereg finansowych niepowodzeń.

Pierwszą oznaką zmiany pogody było nagłe załamanie rynku specjalistycznego sprzętu sztucznej inteligencji w 1987 roku.
Komputery stacjonarne Apple i IBM stopniowo zdobywały szybkość i moc, aw 1987 roku stały się potężniejsze niż droższe
 maszyny Lisp wyprodukowane przez Symbolika i inne. Nie było już powodu, by je kupować. Cały przemysł wart pół miliarda
  dolarów został zniszczony z dnia na dzień.

Ostatecznie najwcześniejsze udane systemy eksperckie, takie jak XCON , okazały się zbyt drogie w utrzymaniu. Trudno je
 było zaktualizować, nie mogły się uczyć, były " kruche " (mogły popełniać groteskowe błędy przy nietypowych nakładach)
  i padały ofiarą problemów (takich jak problem kwalifikacji ), które zostały zidentyfikowane wiele lat wcześniej.
  Systemy ekspertowe okazały się przydatne, ale tylko w kilku szczególnych kontekstach.

Pod koniec lat 80. Strategiczna Inicjatywa Obliczeniowa ograniczyła finansowanie AI "głęboko i brutalnie". Nowe kierownictwo
DARPA zdecydowało, że sztuczna inteligencja nie jest "kolejną falą" i kierowała fundusze na projekty, które wydawałyby się
 bardziej prawdopodobne, że przyniosą natychmiastowe rezultaty.

Do 1991 r. Imponująca lista celów zapisanych w 1981 r. W japońskim projekcie piątej generacji nie została dotrzymana.
W rzeczywistości niektóre z nich, jak "prowadzić swobodną rozmowę", nie zostały spełnione do 2010 r. [130] Podobnie jak
 w przypadku innych projektów sztucznej inteligencji, oczekiwania były znacznie wyższe niż to, co było faktycznie możliwe.
Znaczenie posiadania ciała: nowatorska sztuczna inteligencja i ucieleśniony rozum
Główne artykuły: Nowa sztuczna inteligencja , sztuczna sztuczna inteligencja zorientowana na zachowanie , usytuowana i
 ucieleśniona nauka kognitywna

Pod koniec lat 80. kilku badaczy opowiadało się za całkowicie nowym podejściem do sztucznej inteligencji opartej na
 robotyce. Uważali, że aby pokazać prawdziwą inteligencję, maszyna musi mieć ciało - musi postrzegać, poruszać się,
 przetrwać i radzić sobie ze światem. Argumentowali oni, że te zdolności czuciowo-ruchowe są niezbędne dla wyższych
 umiejętności, takich jak zdroworozsądkowe rozumowanie, i że abstrakcyjne rozumowanie było faktycznie najmniej interesującą
 lub ważną ludzką umiejętnością (patrz paradoks Moraveca ). Opowiadali się za budowaniem inteligencji "od podstaw".

To podejście ożywiło idee cybernetyki i teorii sterowania , które od lat sześćdziesiątych były niepopularne. Innym
prekursorem był David Marr , który przybył do MIT pod koniec lat 70. XX wieku z udanego tła teoretycznej neuronauki,
aby poprowadzić grupę badającą wizję . Odrzucił wszystkie symboliczne podejścia ( zarówno logikę McCarthy'ego, jak i
ramy Minsky'ego ), argumentując, że sztuczna inteligencja musi zrozumieć fizyczną maszynerię widzenia od dołu, zanim
nastąpi jakiekolwiek symboliczne przetwarzanie. (Praca Marra została przerwana przez białaczkę w 1980 r.)

W artykule z 1990 roku "Elephants Do not Play Chess" badacz robotyki Rodney Brooks bezpośrednio zmierzył się z
 hipotezą fizycznego systemu symboli , argumentując, że symbole nie zawsze są konieczne, ponieważ "świat jest jego
 własnym najlepszym modelem. zawsze dokładnie aktualny, zawsze ma każdy szczegół, o którym trzeba wiedzieć, dlatego
 trzeba go wyczuwać odpowiednio i często. W latach osiemdziesiątych i dziewięćdziesiątych wielu naukowców
  kognitywistów odrzuciło także model przetwarzania symboli umysłu i argumentowało, że ciało było niezbędne do
  rozumowania, teorii zwanej wcieloną tezą umysłu.
    </div>
    <div class="">
      <h2>AI 1993-2011</h2>

      Dziedzina sztucznej inteligencji, która miała ponad pół wieku, w końcu osiągnęła niektóre z najstarszych celów.
      Zaczęło być z powodzeniem stosowane w przemyśle technologicznym, choć trochę za kulisami. Niektóre sukcesy
       wynikały ze zwiększenia mocy komputera, a niektóre zostały osiągnięte poprzez skupienie się na konkretnych
        izolowanych problemach i dążenie do nich przy zachowaniu najwyższych standardów odpowiedzialności naukowej.
         Mimo to, reputacja AI, przynajmniej w świecie biznesu, była nieskazitelna. Wewnątrz pola niewiele było
          porozumienia co do powodów niespełnienia przez AI marzenia o inteligencji na poziomie ludzkim, która
          uchwyciła wyobraźnię świata w latach sześćdziesiątych. Wszystkie te czynniki razem przyczyniły się do
           rozdrobnienia sztucznej inteligencji na konkurencyjne subpody koncentrujące się na konkretnych problemach
            lub podejściach, czasami nawet pod nowymi nazwami, które zamaskowały zmącony rodowód "sztucznej inteligencji".
             AI była zarówno ostrożniejsza, jak i bardziej skuteczna niż kiedykolwiek wcześniej.
Kamienie milowe i prawo Moore'a

W dniu 11 maja 1997 r. Deep Blue stał się pierwszym komputerowym systemem do gry w szachy, który pokonał obecnego mistrza
świata w szachach, Garry'ego Kasparowa. Super komputer był wyspecjalizowaną wersją frameworka wyprodukowanego przez IBM
 i był w stanie przetworzyć dwa razy więcej ruchów na sekundę niż podczas pierwszego meczu (który utracił Deep Blue), podobno
  200 000 000 ruchów na sekundę. Wydarzenie transmitowane było na żywo w Internecie i otrzymało ponad 74 miliony odsłon.

W 2005 roku robot Stanford wygrał Grand Challenge DARPA , jeżdżąc autonomicznie przez 131 mil wzdłuż pustego szlaku pustynnego.
  Dwa lata później ekipa z CMU wygrała DARPA Urban Challenge , autonomicznie poruszając się w promieniu 55 mil w środowisku
  miejskim, jednocześnie przestrzegając przepisów drogowych i wszystkich przepisów ruchu drogowego. W lutym 2011 r. W
   Zagrożeniu! mecz wystawowy w quizie , system odpowiadania na pytania , Watson , pokonał dwóch największych Jeopardy!
    mistrzowie, Brad Rutter i Ken Jennings , z dużym marginesem.

Sukcesy te nie były spowodowane rewolucyjnym nowym paradygmatem, ale głównie żmudnym zastosowaniem umiejętności inżynierskich
i ogromną mocą dzisiejszych komputerów. W rzeczywistości komputer Deep Blue był 10 milionów razy szybszy niż Ferranti
Mark 1, którego Christopher Strachey nauczał grać w szachy w 1951 roku. Ten dramatyczny wzrost mierzy prawo Moore'a ,
które przewiduje, że prędkość i pojemność pamięci komputery podwajają się co dwa lata. Zasadniczy problem "surowej mocy
komputera" powoli został przezwyciężony.
Inteligentni agenci

Nowy paradygmat o nazwie " inteligentni agenci " stał się powszechnie akceptowany w latach 90. Chociaż wcześniejsi
naukowcy proponowali modularne podejście "dziel i rządź" do AI, inteligentny agent nie osiągnął swojej nowoczesnej
formy, dopóki Judea Pearl , Allen Newell , Leslie P. Kaelbling i inni nie przynieśli koncepcji z teorii decyzji i ekonomia
 w badaniu AI. Kiedy ekonomista zdefiniował racjonalnego agenta poślubionego przez komputerową definicję obiektu lub
 modułu , paradygmat inteligentnego agenta był kompletny.

Inteligentny agent to system, który dostrzega jego otoczenie i podejmuje działania, które zwiększają jego szanse na sukces.
 Dzięki tej definicji proste programy rozwiązujące określone problemy to "inteligentni agenci", podobnie jak ludzie i
 organizacje ludzi, takie jak firmy . Paradygmat inteligentnego agenta definiuje badania nad sztuczną inteligencją jako
  "badanie inteligentnych agentów". Jest to uogólnienie niektórych wcześniejszych definicji SI: wykracza poza badanie
  ludzkiej inteligencji; bada wszystkie rodzaje inteligencji.

Paradygmat dawał badaczom licencję na badanie pojedynczych problemów i znajdowanie rozwiązań, które były zarówno
 weryfikowalne, jak i użyteczne. Zapewnił wspólny język do opisywania problemów i dzielenia się swoimi rozwiązaniami
  ze sobą, a także z innymi dziedzinami, które wykorzystywały koncepcje czynników abstrakcyjnych, takich jak ekonomia
   i teoria sterowania . Miano nadzieję, że pełna architektura agentów (np . SOAR firmy Newell ) pewnego dnia umożliwi
    naukowcom budowanie bardziej wszechstronnych i inteligentnych systemów spośród interaktywnych inteligentnych
     agentów.
"Victory of the neats"

Naukowcy z AI zaczęli rozwijać i używać wyrafinowanych narzędzi matematycznych bardziej niż kiedykolwiek w przeszłości.
 Powszechnie zdawano sobie sprawę, że wiele problemów, które AI musiała rozwiązać, było już opracowywanych przez
naukowców w dziedzinach takich jak matematyka , ekonomia czy badania operacyjne . Wspólny język matematyczny pozwolił
zarówno na wyższy poziom współpracy z bardziej ugruntowanymi i odnoszącymi sukcesy obszarami, jak i na osiągnięcie
wyników, które były mierzalne i możliwe do udowodnienia; AI stała się bardziej rygorystyczną "naukową" dyscypliną.
Russell i Norvig (2003) opisują to jako "rewolucję" i "zwycięstwo schludności".

Bardzo wpływowa książka Judei Pearl z 1988 roku wprowadziła teorię prawdopodobieństwa i decyzji do sztucznej
 inteligencji. Wśród wielu nowych narzędzi w użyciu były sieci bayesowskie , ukryte modele Markowa , teoria informacji ,
 modelowanie stochastyczne i klasyczna optymalizacja . Opracowano również precyzyjne opisy matematyczne dla paradygmatów "
  obliczeniowej inteligencji ", takich jak sieci neuronowe i algorytmy ewolucyjne.
AI za kulisami

Algorytmy pierwotnie opracowane przez badaczy AI zaczęły pojawiać się jako części większych systemów. AI rozwiązało wiele
 bardzo trudnych problemów, a ich rozwiązania okazały się przydatne w całej branży technologicznej, takich jak
  eksploracja danych , robotyka przemysłowa , logistyka, rozpoznawanie mowy , oprogramowanie bankowe,
   diagnoza medyczna i wyszukiwarka Google.

Sfera AI otrzymała niewielki lub żaden kredyt za te sukcesy w latach 90. i na początku 2000 roku. Wiele największych
 innowacji AI zostało zredukowanych do statusu tylko jednego elementu w skrzyni narzędziowej informatyki. Nick
  Bostrom wyjaśnia: "Wiele najnowocześniejszych sztucznej inteligencji przełożyło się na ogólne aplikacje, często bez
  nazywania sztucznej inteligencji, ponieważ gdy coś stanie się wystarczająco przydatne i powszechne, nie będzie już
  oznaczane jako AI".

Wielu badaczy w sztucznej inteligencji w latach 90. celowo nazywało ich pracę innymi nazwami, takimi jak informatyka ,
 systemy oparte na wiedzy, systemy kognitywne lub inteligencja obliczeniowa . Częściowo może to wynikać z faktu, że ich
 środowisko zasadniczo różni się od sztucznej inteligencji, ale również nowe nazwy pomagają zdobyć fundusze. W świecie
 komercyjnym przynajmniej nieudane obietnice AI Winter nadal nawiedzają badania nad sztuczną inteligencją w latach 2000,
  jak napisał New York Times w 2005 roku: "Informatycy i inżynierowie oprogramowania unikali terminu" sztuczna inteligencja
  "z obawy przed byciem postrzeganym jako dziki -uczeni marzyciele. "
Gdzie jest HAL 9000?

W 1968 roku Arthur C. Clarke i Stanley Kubrick wyobrażali sobie, że do 2001 roku maszyna będzie istnieć z inteligencją,
która pasuje lub przewyższa możliwości człowieka. Postać, którą stworzyli, HAL 9000 , została oparta na przekonaniu wielu
 czołowych badaczy sztucznej inteligencji, że taka maszyna istniałaby do 2001 roku.

W 2001 roku, założyciel AI, Marvin Minsky, zapytał: "Więc pytanie brzmi: dlaczego nie uzyskaliśmy HAL w 2001 roku?"
Minsky sądził, że odpowiedź brzmi: zaniedbywano główne problemy, takie jak zdroworozsądkowe rozumowanie , podczas
 gdy większość badaczy zajmowała się komercyjnymi zastosowaniami sieci neuronowych lub algorytmów genetycznych . Z kolei
  John McCarthy nadal obwiniał problem kwalifikacji. Dla Raya Kurzweila problemem jest siła komputera i, używając
   Prawa Moore'a , przewidział, że maszyny z inteligencją na poziomie ludzkim pojawią się przed 2029 r. [168] Jeff Hawkins
    twierdził, że badania neuronowe ignorują istotne właściwości ludzkiej kory mózgowej , preferując proste modele, które
     odniosły sukces w rozwiązywaniu prostych problemów. Było wiele innych wyjaśnień, a dla każdego z nich prowadzono
      odpowiedni program badawczy.
    </div>
    <div class="">
      <h2>Głębokie nauczanie, big data - AI w latach 2011–obecnie</h2>

      W pierwszych dziesięcioleciach XXI wieku dostęp do dużych ilości danych (znanych jako " duże dane "), szybszych
      komputerów i zaawansowanych technik uczenia maszynowego z powodzeniem zastosowano w wielu problemach w całej gospodarce.
      W rzeczywistości McKinsey Global Institute oszacował w swoim słynnym artykule "Big data: następna granica innowacji,
      konkurencji i produktywności", że "do 2009 r. Prawie wszystkie sektory w amerykańskiej gospodarce miały co najmniej 200
       terabajtów zgromadzonych danych" .

Do 2016 roku rynek produktów, sprzętu i oprogramowania związanych z sztuczną inteligencją osiągnął ponad 8 miliardów dolarów,
 a New York Times poinformował, że zainteresowanie AI osiągnęło "szał". Zastosowania dużych zbiorów danych zaczęły
 również docierać do innych dziedzin, takich jak modele szkoleniowe w ekologii i do różnych zastosowań ekonomicznych .
  Postępy w głębokim uczeniu się (szczególnie głębokie splotowe sieci neuronowe i nawracające sieci neuronowe )
 doprowadziły do ​​postępu i badań w zakresie przetwarzania obrazu i wideo, analizy tekstu, a nawet rozpoznawania mowy.
Głębokie kształcenie
Główny artykuł: Głęboka nauka

Głębokie uczenie się jest gałęzią uczenia maszynowego, która modeluje abstrakcje wysokiego poziomu w danych za pomocą
głębokiego wykresu z wieloma warstwami przetwarzania. Zgodnie z uniwersalnym twierdzeniem o przybliżeniu , głębia
 nie jest konieczna, aby sieć neuronowa mogła przybliżać dowolne ciągłe funkcje. Mimo to istnieje wiele problemów, które
 są wspólne dla płytkich sieci (takich jak przeuczenie ), których unikają głębokie sieci. W związku z tym głębokie
  sieci neuronowe są w stanie realistycznie wygenerować o wiele bardziej złożone modele w porównaniu do ich płytkich
   odpowiedników.

Jednak głębokie uczenie się ma własne problemy. Powszechnym problemem dotyczącym nawracających sieci neuronowych jest
problem zanikającego gradientu , który polega na tym, że gradienty przechodzące między warstwami stopniowo kurczą się i
dosłownie znikają, gdy są zaokrąglane do zera. Opracowano wiele metod podejścia do tego problemu, takich jak długoterminowe
jednostki pamięci krótkotrwałej .

Nowoczesne , głębokie architektury sieci neuronowych mogą czasami rywalizować z ludzką dokładnością w dziedzinach takich jak
 wizja komputera, w szczególności w takich dziedzinach jak baza danych MNIST i rozpoznawanie znaków drogowych.

Silniki do przetwarzania języka, napędzane przez inteligentne wyszukiwarki, mogą łatwo pokonać ludzi w odpowiedzi na ogólne
pytania dotyczące trivia (takie jak IBM Watson ), a najnowsze osiągnięcia w zakresie głębokiego uczenia się dały zdumiewające
 wyniki w rywalizacji z ludźmi, w takich rzeczach jak Go and Doom (który, będąc FPS, wywołał pewne kontrowersje).

Big Data

Duże dane odnoszą się do zbioru danych, które nie mogą być przechwytywane, zarządzane i przetwarzane przez konwencjonalne
narzędzia programowe w określonym przedziale czasowym. Jest to ogromna ilość możliwości podejmowania decyzji, wglądu i
optymalizacji procesów, które wymagają nowych modeli przetwarzania. W Big Data Era autorstwa Victora Meyera Schonberga i
 Kennetha Cooke'a duże zbiory danych oznaczają, że zamiast analizy losowej (badanie próbki) wszystkie dane są wykorzystywane
 do analizy. Charakterystyka dużych danych 5V (proponowana przez IBM): objętość , prędkość , różnorodność, wartość
  , dokładność. Strategiczne znaczenie technologii big data nie polega na opanowaniu ogromnych danych, ale na
   specjalizacji w tych znaczących danych. Innymi słowy, jeśli duże dane są porównywane do branży, kluczem do uzyskania
   rentowności w tej branży jest zwiększenie " zdolności procesu " danych i realizacja " wartości dodanej " danych poprzez "
    przetwarzanie ".


Sztuczna inteligencja ogólna

Sztuczna inteligencja jest gałęzią informatyki, która próbuje zrozumieć istotę inteligencji i wytworzyć nową inteligentną
maszynę, która reaguje w sposób podobny do ludzkiej inteligencji. Badania w tej dziedzinie obejmują robotykę , rozpoznawanie
mowy, rozpoznawanie obrazów , przetwarzanie języka naturalnego i systemy eksperckie. Od narodzin sztucznej inteligencji teoria
 i technologia stają się coraz dojrzalsze, a pola zastosowań rozszerzają się. Można sobie wyobrazić, że produkty
  technologiczne przynoszone przez sztuczną inteligencję będą w przyszłości "pojemnikiem" ludzkiej mądrości. Sztuczna
  inteligencja może symulować proces informacji ludzkiej świadomości i myślenia. Sztuczna inteligencja nie jest ludzką
  inteligencją, ale może być jak ludzkie myślenie i może przekraczać ludzką inteligencję. Sztuczna inteligencja ogólna
  określana jest również jako " silna sztuczna inteligencja ", " pełna sztuczna inteligencja " lub zdolność
  maszyny do wykonywania "ogólnego inteligentnego działania". Źródła naukowe rezerwują "silną sztuczną inteligencję"
   w odniesieniu do maszyn zdolnych do odczuwania świadomości.

   źródło wikipedia.pl
    </div>
{% endblock %}
